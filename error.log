"C:\Program Files\Java\jdk1.8.0_141\bin\java.exe" -javaagent:C:\Users\Rahul_Raj05\Downloads\ideaIC-2018.1.1.win\lib\idea_rt.jar=54837:C:\Users\Rahul_Raj05\Downloads\ideaIC-2018.1.1.win\bin -Dfile.encoding=UTF-8 -classpath C:\Users\Rahul_Raj05\AppData\Local\Temp\classpath1326472045.jar SantanderValuePrediction
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/C:/Users/Rahul_Raj05/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/C:/Users/Rahul_Raj05/.m2/repository/ch/qos/logback/logback-classic/1.1.3/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/C:/Users/Rahul_Raj05/.m2/repository/org/slf4j/slf4j-simple/1.7.25/slf4j-simple-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/06/21 21:20:44 INFO SparkContext: Running Spark version 2.1.0
18/06/21 21:20:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/06/21 21:20:46 INFO SecurityManager: Changing view acls to: Rahul_Raj05
18/06/21 21:20:46 INFO SecurityManager: Changing modify acls to: Rahul_Raj05
18/06/21 21:20:46 INFO SecurityManager: Changing view acls groups to: 
18/06/21 21:20:46 INFO SecurityManager: Changing modify acls groups to: 
18/06/21 21:20:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Rahul_Raj05); groups with view permissions: Set(); users  with modify permissions: Set(Rahul_Raj05); groups with modify permissions: Set()
18/06/21 21:20:47 INFO Utils: Successfully started service 'sparkDriver' on port 54858.
18/06/21 21:20:47 INFO SparkEnv: Registering MapOutputTracker
18/06/21 21:20:47 INFO SparkEnv: Registering BlockManagerMaster
18/06/21 21:20:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/21 21:20:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/21 21:20:47 INFO DiskBlockManager: Created local directory at C:\Users\Rahul_Raj05\AppData\Local\Temp\blockmgr-e836796c-4bf4-4619-a53b-a444c770210b
18/06/21 21:20:47 INFO MemoryStore: MemoryStore started with capacity 896.4 MB
18/06/21 21:20:48 INFO SparkEnv: Registering OutputCommitCoordinator
18/06/21 21:20:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/06/21 21:20:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.56.1:4040
18/06/21 21:20:48 INFO Executor: Starting executor ID driver on host localhost
18/06/21 21:20:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54879.
18/06/21 21:20:48 INFO NettyBlockTransferService: Server created on 192.168.56.1:54879
18/06/21 21:20:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/21 21:20:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.56.1, 54879, None)
18/06/21 21:20:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.56.1:54879 with 896.4 MB RAM, BlockManagerId(driver, 192.168.56.1, 54879, None)
18/06/21 21:20:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.56.1, 54879, None)
18/06/21 21:20:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.56.1, 54879, None)
18/06/21 21:20:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.1 KB, free 896.3 MB)
18/06/21 21:20:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.3 KB, free 896.3 MB)
18/06/21 21:20:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.56.1:54879 (size: 14.3 KB, free: 896.4 MB)
18/06/21 21:20:50 INFO SparkContext: Created broadcast 0 from textFile at SantanderValuePrediction.java:50
Exception in thread "main" org.apache.spark.SparkException: Task not serializable
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)
	at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:369)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.map(RDD.scala:369)
	at org.apache.spark.api.java.JavaRDDLike$class.map(JavaRDDLike.scala:93)
	at org.apache.spark.api.java.AbstractJavaRDDLike.map(JavaRDDLike.scala:45)
	at SantanderValuePrediction.main(SantanderValuePrediction.java:51)
Caused by: java.io.NotSerializableException: org.datavec.api.split.FileSplit
Serialization stack:
	- object not serializable (class: org.datavec.api.split.FileSplit, value: org.datavec.api.split.FileSplit@690bc15e)
	- field (class: org.datavec.api.records.reader.impl.LineRecordReader, name: inputSplit, type: interface org.datavec.api.split.InputSplit)
	- object (class org.datavec.api.records.reader.impl.csv.CSVRecordReader, org.datavec.api.records.reader.impl.csv.CSVRecordReader@71b6d77f)
	- field (class: org.datavec.api.records.reader.impl.transform.TransformProcessRecordReader, name: recordReader, type: interface org.datavec.api.records.reader.RecordReader)
	- object (class org.datavec.api.records.reader.impl.transform.TransformProcessRecordReader, org.datavec.api.records.reader.impl.transform.TransformProcessRecordReader@126f1ba8)
	- field (class: org.datavec.spark.transform.misc.StringToWritablesFunction, name: recordReader, type: interface org.datavec.api.records.reader.RecordReader)
	- object (class org.datavec.spark.transform.misc.StringToWritablesFunction, org.datavec.spark.transform.misc.StringToWritablesFunction@2b037cfc)
	- field (class: org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, name: fun$1, type: interface org.apache.spark.api.java.function.Function)
	- object (class org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, <function1>)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)
	... 12 more
18/06/21 21:20:50 INFO SparkContext: Invoking stop() from shutdown hook
18/06/21 21:20:50 INFO SparkUI: Stopped Spark web UI at http://192.168.56.1:4040
18/06/21 21:20:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/06/21 21:20:50 INFO MemoryStore: MemoryStore cleared
18/06/21 21:20:50 INFO BlockManager: BlockManager stopped
18/06/21 21:20:50 INFO BlockManagerMaster: BlockManagerMaster stopped
18/06/21 21:20:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/06/21 21:20:50 INFO SparkContext: Successfully stopped SparkContext
18/06/21 21:20:50 INFO ShutdownHookManager: Shutdown hook called
18/06/21 21:20:50 INFO ShutdownHookManager: Deleting directory C:\Users\Rahul_Raj05\AppData\Local\Temp\spark-a2c53ed7-f7b9-4039-b927-83e6bc2f1f64

Process finished with exit code 1
